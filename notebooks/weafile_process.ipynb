{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather file - gap filling and formatting\n",
    "- Main tasks: \n",
    "    1. gap fill weather file\n",
    "    2. combine met and solrad info into single files for each site-year\n",
    "    3. address timezone issue\n",
    "    4. format weather file into MAIZSIM-readable format\n",
    "- Data source: \n",
    "    1. weadata/**temp_all.csv**\n",
    "    2. weadata/**rh_all.csv**\n",
    "    3. weadata/**precip_all.csv**\n",
    "    4. weadata/**solrad_all.csv**\n",
    "- Main output: \n",
    "    - weadata/data/control/**site_year.txt** - weather file for all site-years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import time \n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "from palettable.colorbrewer.sequential import OrRd_6\n",
    "from palettable.colorbrewer.sequential import YlGn_9\n",
    "from palettable.colorbrewer.sequential import YlGnBu_8\n",
    "from palettable.colorbrewer.sequential import RdPu_5\n",
    "\n",
    "#from funcs import find_zone, utc_to_local, CC_VPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Reading in temperature, precip, RH & solar radiation data:\n",
    "- Main input:\n",
    "    - /weadata/**temp_all.csv**\n",
    "    - /weadata/**rh_all.csv**\n",
    "    - /weadata/**precip_all.csv**\n",
    "    - /weadata/**solrad_all.csv**\n",
    "- Main output: \n",
    "    - **df_temp, df_rh, df_precip, df_solrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Read in weather data 1961-1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198000, 237)\n",
      "(198000, 237)\n",
      "(198000, 237)\n",
      "(198000, 237)\n"
     ]
    }
   ],
   "source": [
    "# read in individual weather data\n",
    "df_temp_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/temp_6190.csv', index_col=0)\n",
    "df_rh_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/rh_6190.csv', index_col=0)\n",
    "df_precip_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/precip_6190.csv', index_col=0)\n",
    "df_solrad_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_6190.csv', index_col=0)\n",
    "\n",
    "# re-index solar radiation data to only include growing season\n",
    "df_solrad_6190 = df_solrad_6190.reindex(df_temp_6190.index)\n",
    "\n",
    "# check that all met elements aligned - dataframe shape should match\n",
    "print(df_temp_6190.shape)\n",
    "print(df_rh_6190.shape)\n",
    "print(df_precip_6190.shape)\n",
    "print(df_solrad_6190.shape)\n",
    "\n",
    "# convert station ID header from WBAN to USAF (in order to make continuous with 1991-2010)\n",
    "df_stations = pd.read_csv('/home/disk/eos8/ach315/data/ISH_NSRD/stations_wban_usaf.csv', header=None, dtype='str')\n",
    "df_stations.columns = ['WBAN', 'USAF']\n",
    "sites_wban = list(df_temp_6190.columns)\n",
    "sites_usaf = df_stations[df_stations['WBAN'].isin(sites_wban)]['USAF']\n",
    "\n",
    "# assign new USAF headers\n",
    "df_temp_6190.columns = sites_usaf; df_temp_6190 = df_temp_6190.sort_index(axis=1)\n",
    "df_rh_6190.columns = sites_usaf; df_rh_6190 = df_rh_6190.sort_index(axis=1)\n",
    "df_precip_6190.columns = sites_usaf; df_precip_6190 = df_precip_6190.sort_index(axis=1)\n",
    "df_solrad_6190.columns = sites_usaf; df_solrad_6190 = df_solrad_6190.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Read in weather data 1991-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132000, 241)\n",
      "(132000, 241)\n",
      "(132000, 241)\n",
      "(132000, 242)\n"
     ]
    }
   ],
   "source": [
    "## read in individual weather data\n",
    "df_temp_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/temp_9110_class1.csv', index_col=0)\n",
    "df_rh_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/rh_9110_class1.csv', index_col=0)\n",
    "df_precip_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/precip_9110_class1.csv', index_col=0)\n",
    "df_solrad_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/solrad_9110_class1.csv', index_col=0)\n",
    "\n",
    "# re-index solar radiation data to only include growing season\n",
    "df_solrad_9110 = df_solrad_9110.reindex(df_temp_9110.index)\n",
    "\n",
    "# check that all met elements aligned - dataframe shape should match\n",
    "print(df_temp_9110.shape)\n",
    "print(df_rh_9110.shape)\n",
    "print(df_precip_9110.shape)\n",
    "print(df_solrad_9110.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Stitch together weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330000, 274)\n",
      "(330000, 274)\n",
      "(330000, 274)\n",
      "(330000, 274)\n"
     ]
    }
   ],
   "source": [
    "df_temp = pd.concat([df_temp_6190, df_temp_9110], axis=0, join='outer'); df_temp = df_temp.sort_index(axis=1)\n",
    "df_rh = pd.concat([df_rh_6190, df_rh_9110], axis=0, join='outer'); df_rh = df_rh.sort_index(axis=1)\n",
    "df_precip = pd.concat([df_precip_6190, df_precip_9110], axis=0, join='outer'); df_precip = df_precip.sort_index(axis=1)\n",
    "df_solrad = pd.concat([df_solrad_6190, df_solrad_9110], axis=0, join='outer'); df_solrad = df_solrad.sort_index(axis=1)\n",
    "\n",
    "print(df_temp.shape)\n",
    "print(df_rh.shape)\n",
    "print(df_precip.shape)\n",
    "print(df_solrad.shape)\n",
    "\n",
    "#df_temp.to_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all.csv')\n",
    "#df_rh.to_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all.csv')\n",
    "#df_precip.to_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all.csv')\n",
    "#df_solrad.to_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Figure out valid site-years that can be gap-filled\n",
    "Selecting for site-years based on **crit_hrs** - consecutive missing hours of datapoints within raw data\n",
    "- Main input: **df_temp, df_precip, df_solrad**\n",
    "- Main output: **finalist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 89 ms, total: 3min 32s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# input variables for loop\n",
    "datasets = list([df_temp, df_precip, df_solrad]) # weather datasets to process\n",
    "                                                 # df_rh is based off df_temp, so no need to evaluate \n",
    "\n",
    "finalist = list([[], [], []]) # final lists to store processed output\n",
    "                              # order: [0]-temp, [1]-precip, [2]-solrad\n",
    "\n",
    "years = np.arange(1961, 2011) # years\n",
    "growseason_start = '-03-01 00:00:00'\n",
    "growseason_end = '-11-30 23:00:00' \n",
    "\n",
    "crit_hrs = 2 # critical hrs of missing data\n",
    "\n",
    "# loop through temp, precip & solrad dataset to pick out usable site-years\n",
    "for i in np.arange(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    siteyears_all = list()\n",
    "    sites = dataset.columns\n",
    "    \n",
    "    for j in years:\n",
    "        start_time = str(j) + growseason_start\n",
    "        end_time = str(j) + growseason_end\n",
    "        siteyears = list()\n",
    "        \n",
    "        for k in sites:\n",
    "            df = dataset.loc[start_time:end_time, k] \n",
    "            df = pd.DataFrame(df)\n",
    "            df['group'] = df.notnull().astype(int) # df.notnull() returns TRUE or FALSE, \n",
    "                                                   # .astype(int) turns TRUE into 1, and FALSE into 0\n",
    "            df['group'] = df.group.cumsum() # calculating cumulative sum \n",
    "            df = df[df.iloc[:,0].isnull()] # selecting out individual timesteps that have missing data\n",
    "            nans_list = df.groupby('group')['group'].count() # counts the number of consecutive NANs \n",
    "            if nans_list[nans_list > crit_hrs].shape[0] == 0:\n",
    "                use_siteyear = str(j) + '_' + str(k)\n",
    "                siteyears.append(use_siteyear) # only record site-years that have fewer consecutive NANs than the critical value set\n",
    "\n",
    "            # The logic of this section of code:\n",
    "            # If weadata is absent (df.notnull == FALSE) you get a return of 0, thus,\n",
    "            # df.group.cumsum() would not change the cumulative sum when encountering NANs since you're only adding 0.\n",
    "            # By doing so, you end up with repeated cumsum() values when you have multiple NANs following it.\n",
    "            # cumsum() values are documented in the 'group' column.\n",
    "            # groupby('group') allows you to then group the cumsum() values into groups and document their counts. \n",
    "            # If a specific cumsum() values has counts greater than 1, that means there were NAN values that followed it.\n",
    "            # The code then evaluates whether there were consecutive NAN values that exceeded the designated critical values.\n",
    "            # If so, that site-years is excluded. \n",
    "        \n",
    "        siteyears_all.extend(siteyears)\n",
    "    \n",
    "    finalist[i] = siteyears_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Compare usable site-years for temp  & precip and find the common year-sites\n",
    "- Main intput: **finalist**\n",
    "- Main output: **siteyears**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 2331\n",
      "precip: 1854\n",
      "solrad: 11937\n",
      "overlapping siteyears: 1673\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 0\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr0 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 4298\n",
      "precip: 3154\n",
      "solrad: 11937\n",
      "overlapping siteyears: 3021\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 1\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr1 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 6096\n",
      "precip: 4397\n",
      "solrad: 11937\n",
      "overlapping siteyears: 4225\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 2\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr2 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 6254\n",
      "precip: 4564\n",
      "solrad: 11937\n",
      "overlapping siteyears: 4398\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 3\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr3 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Store basic info of valid site-years\n",
    "- Main input: **siteyears_crithr2**\n",
    "    - crithr2 seems to be the best interval given the balance between gaining siteyears vs. limiting gap filling\n",
    "- Main output: \n",
    "    - weadata/**siteyears_crithr2.csv** - site-year info for data filtered with crithr = 2\n",
    "    - weadata/**site_nyears_crithr2.csv** - info on how many years of wea data each site has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siteyears = siteyears_crithr2 \n",
    "\n",
    "# what are the valid site-years?\n",
    "years = list()\n",
    "sites = list()\n",
    "\n",
    "for i in range(len(siteyears)):\n",
    "    year = siteyears[i][0:4]\n",
    "    years.append(year)\n",
    "    site = siteyears[i][5:11] \n",
    "    sites.append(site)\n",
    "\n",
    "df_siteyears = pd.DataFrame({'site': sites, 'year': years}, \n",
    "                            columns=['site', 'year'])\n",
    "df_siteyears = df_siteyears.sort_values(['site', 'year'])\n",
    "final_sites = list(set(df_siteyears.site))\n",
    "\n",
    "# how many years of data do each site have?\n",
    "site_nyears = list()\n",
    "\n",
    "for i in final_sites:\n",
    "    years = len(df_siteyears[df_siteyears[\"site\"] == i])\n",
    "    site_nyears.append(years)\n",
    "    \n",
    "df_site_nyears = pd.DataFrame({\"site\": final_sites, \"years\": site_nyears})\n",
    "df_site_nyears = df_site_nyears.sort_values([\"site\"])\n",
    "df_site_nyears = df_site_nyears.reset_index().iloc[:, 1:3]\n",
    "\n",
    "# writing out info as .csv\n",
    "#df_siteyears.to_csv('../weadata/siteyears_crithr2.csv')\n",
    "#df_site_nyears.to_csv(\"../weadata/site_nyears_crithr2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter sites based on planting area & irrigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Select sites with valid weather data\n",
    "- Main output: \n",
    "    - **df_sites_info**\n",
    "    - **site_summary.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in station & site-year info\n",
    "df_site_nyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_nyears_crithr2.csv', \n",
    "                             index_col=0, dtype={'site': str})\n",
    "df_stations_9110 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/stations_info_9110.csv', \n",
    "                               dtype={'USAF': str}, usecols=[0,1,3,4,8,9,10])\n",
    "df_sites_info = df_stations_9110[df_stations_9110.USAF.isin(df_site_nyears.site)]\n",
    "df_sites_info.columns = ['site', 'class', 'station', 'state', 'tzone', 'lat', 'lon']\n",
    "\n",
    "# merge site info & site-years info\n",
    "df_sites_info = pd.merge(df_sites_info, df_site_nyears, on='site')\n",
    "\n",
    "# drop stations from Alaska, Guam, Hawaii & Puerto Rico\n",
    "df_sites_info = df_sites_info[(df_sites_info.state != 'AK') & (df_sites_info.state != 'GU') & \n",
    "                              (df_sites_info.state != 'HI')& (df_sites_info.state != 'PR')]\n",
    "\n",
    "# final station list\n",
    "df_sites_info.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Include planting area & irrigation info\n",
    "- Main output: \n",
    "    - df_obs: obs_areairri.csv\n",
    "    - df_summary: site_summary.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Census data of overall planting area & irrigation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/stateID.txt', header=None, sep='\\s+')\n",
    "county = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/countyID.txt', header=None, sep='\\s+')\n",
    "irri = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/irr_area_acres.txt', header=None, sep='\\s+')\n",
    "area = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/crop_area_acres.txt', header=None, sep='\\s+')\n",
    "state = state.iloc[0,:]\n",
    "county = county.iloc[0,:]\n",
    "\n",
    "# raw data includes data from 4 censuses that show data of 1997, 2002, 2007 & 2012\n",
    "# we average data from all 4 censuses \n",
    "irri = irri.mean(axis=1)\n",
    "area = area.mean(axis=1)\n",
    "df_census = pd.DataFrame({'state': state, 'county': county, 'perct_irri': irri/area*100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NASS data of maize planting area & yield for individual years 1961-2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>county_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.462991</td>\n",
       "      <td>-86.709691</td>\n",
       "      <td>1961</td>\n",
       "      <td>2.008561</td>\n",
       "      <td>6758.2607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.787262</td>\n",
       "      <td>-87.712913</td>\n",
       "      <td>1961</td>\n",
       "      <td>2.761771</td>\n",
       "      <td>9024.5038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.870087</td>\n",
       "      <td>-85.383129</td>\n",
       "      <td>1961</td>\n",
       "      <td>1.945793</td>\n",
       "      <td>11452.6210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.072877</td>\n",
       "      <td>-87.112698</td>\n",
       "      <td>1961</td>\n",
       "      <td>2.259631</td>\n",
       "      <td>2092.2280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.988350</td>\n",
       "      <td>-86.613622</td>\n",
       "      <td>1961</td>\n",
       "      <td>2.322398</td>\n",
       "      <td>10064.5470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_id  county_id        lat        lon  year     yield        area\n",
       "0       1.0        1.0  32.462991 -86.709691  1961  2.008561   6758.2607\n",
       "1       1.0        3.0  30.787262 -87.712913  1961  2.761771   9024.5038\n",
       "2       1.0        5.0  31.870087 -85.383129  1961  1.945793  11452.6210\n",
       "3       1.0        7.0  33.072877 -87.112698  1961  2.259631   2092.2280\n",
       "4       1.0        9.0  33.988350 -86.613622  1961  2.322398  10064.5470"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in state & county id\n",
    "state_id = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/stateID.txt\", sep=\"\\s+\", header=None)\n",
    "state_id = state_id.transpose()\n",
    "state_id.columns = [\"state_id\"]\n",
    "county_id = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/countyID.txt\", sep=\"\\s+\", header=None)\n",
    "county_id = county_id.transpose()\n",
    "county_id.columns = [\"county_id\"]\n",
    "\n",
    "# Read in lat & lon information\n",
    "lat_county = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/lat_county.txt\", sep=\"\\s+\", header=None)\n",
    "lat_county = lat_county.transpose()\n",
    "lat_county.columns = [\"lat\"]\n",
    "lon_county = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/lon_county.txt\", sep=\"\\s+\", header=None)\n",
    "lon_county = lon_county.transpose()\n",
    "lon_county.columns = [\"lon\"]\n",
    "\n",
    "# Read in maize yield\n",
    "cornyield = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/corn_yield.txt\", sep=\"\\s+\", header=None)\n",
    "years = np.arange(1910, 2015)\n",
    "cornyield.columns = years\n",
    "\n",
    "# Reading in maize area\n",
    "cornarea = pd.read_csv(\"/home/disk/eos8/ach315/data/NASS_cropdata/corn_area.txt\", sep=\"\\s+\", header=None)\n",
    "years = np.arange(1910, 2015)\n",
    "cornarea.columns = years\n",
    "cornarea.head()\n",
    "cornarea = cornarea.melt(var_name='year', value_name='area')\n",
    "cornarea = cornarea.drop(['year'], axis=1)\n",
    "\n",
    "# concat all info and melt dataframe\n",
    "df = pd.concat([state_id, county_id, lat_county, lon_county, cornyield], axis=1)\n",
    "df = pd.melt(df, id_vars=['state_id', 'county_id', 'lat', 'lon'], value_name='yield', var_name=\"year\")\n",
    "df = pd.concat([df, cornarea], axis=1)\n",
    "\n",
    "# subsetting data for year 1961-2005\n",
    "df_nass = pd.DataFrame()\n",
    "years = np.arange(1961,2006)\n",
    "for i in range(len(years)):\n",
    "    data = df[df['year'] == years[i]]\n",
    "    df_nass = pd.concat([df_nass, data])\n",
    "\n",
    "df_nass = df_nass.reset_index(drop=True)\n",
    "df_nass.year = df_nass.year.astype(int)\n",
    "df_nass.head()\n",
    "#df_nass.to_csv('/home/disk/eos8/ach315/upscale/weadata/obs_nass.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Select maximum planting area for each county within the 1961-2005 time period\n",
    "- Merge planting area info with census data of percentage irrigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_id</th>\n",
       "      <th>county_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>area</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>perct_irri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.462991</td>\n",
       "      <td>-86.709691</td>\n",
       "      <td>6758.2607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.112183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30.787262</td>\n",
       "      <td>-87.712913</td>\n",
       "      <td>19748.6900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.510410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.870087</td>\n",
       "      <td>-85.383129</td>\n",
       "      <td>11452.6210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.945534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.072877</td>\n",
       "      <td>-87.112698</td>\n",
       "      <td>2092.2280</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.624115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.988350</td>\n",
       "      <td>-86.613622</td>\n",
       "      <td>10064.5470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.940536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   state_id  county_id        lat        lon        area  state  county  \\\n",
       "0       1.0        1.0  32.462991 -86.709691   6758.2607    1.0     1.0   \n",
       "1       1.0        3.0  30.787262 -87.712913  19748.6900    1.0     3.0   \n",
       "2       1.0        5.0  31.870087 -85.383129  11452.6210    1.0     5.0   \n",
       "3       1.0        7.0  33.072877 -87.112698   2092.2280    1.0     7.0   \n",
       "4       1.0        9.0  33.988350 -86.613622  10064.5470    1.0     9.0   \n",
       "\n",
       "   perct_irri  \n",
       "0    2.112183  \n",
       "1    8.510410  \n",
       "2    4.945534  \n",
       "3    0.624115  \n",
       "4    0.940536  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nass_group = df_nass.groupby(['state_id', 'county_id'])[['lat', 'lon', 'area']].max()\n",
    "df_nass_group = df_nass_group.reset_index()\n",
    "df_obs = df_nass_group.merge(df_census, how='left', \n",
    "                             left_on=['state_id','county_id'], \n",
    "                             right_on=['state','county'])\n",
    "df_obs.head()\n",
    "#df_obs.to_csv('/home/disk/eos8/ach315/upscale/weadata/obs_areairri.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finds the 5 nearest NASS observations sites next to each simulation site with weather station data\n",
    "- Averages the planting area & irrigation percentage from all the nearest 5 locations\n",
    "- Assigns averaged value to each simulation site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>class</th>\n",
       "      <th>station</th>\n",
       "      <th>state</th>\n",
       "      <th>tzone</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>years</th>\n",
       "      <th>area</th>\n",
       "      <th>perct_irri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>722010</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY WEST INTL ARPT</td>\n",
       "      <td>FL</td>\n",
       "      <td>-5</td>\n",
       "      <td>24.550</td>\n",
       "      <td>-81.750</td>\n",
       "      <td>25</td>\n",
       "      <td>101.171570</td>\n",
       "      <td>60.802507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>722020</td>\n",
       "      <td>1</td>\n",
       "      <td>MIAMI INTL AP</td>\n",
       "      <td>FL</td>\n",
       "      <td>-5</td>\n",
       "      <td>25.817</td>\n",
       "      <td>-80.300</td>\n",
       "      <td>29</td>\n",
       "      <td>374.334800</td>\n",
       "      <td>61.901377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>722030</td>\n",
       "      <td>1</td>\n",
       "      <td>WEST PALM BEACH INTL ARPT</td>\n",
       "      <td>FL</td>\n",
       "      <td>-5</td>\n",
       "      <td>26.683</td>\n",
       "      <td>-80.100</td>\n",
       "      <td>19</td>\n",
       "      <td>647.498030</td>\n",
       "      <td>78.435473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>722050</td>\n",
       "      <td>1</td>\n",
       "      <td>ORLANDO INTL ARPT</td>\n",
       "      <td>FL</td>\n",
       "      <td>-5</td>\n",
       "      <td>28.433</td>\n",
       "      <td>-81.333</td>\n",
       "      <td>19</td>\n",
       "      <td>607.029392</td>\n",
       "      <td>55.098924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>722056</td>\n",
       "      <td>1</td>\n",
       "      <td>DAYTONA BEACH INTL AP</td>\n",
       "      <td>FL</td>\n",
       "      <td>-5</td>\n",
       "      <td>29.183</td>\n",
       "      <td>-81.067</td>\n",
       "      <td>19</td>\n",
       "      <td>558.467041</td>\n",
       "      <td>50.088347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     site  class                    station state  tzone     lat     lon  \\\n",
       "0  722010      1         KEY WEST INTL ARPT    FL     -5  24.550 -81.750   \n",
       "1  722020      1              MIAMI INTL AP    FL     -5  25.817 -80.300   \n",
       "2  722030      1  WEST PALM BEACH INTL ARPT    FL     -5  26.683 -80.100   \n",
       "3  722050      1          ORLANDO INTL ARPT    FL     -5  28.433 -81.333   \n",
       "4  722056      1      DAYTONA BEACH INTL AP    FL     -5  29.183 -81.067   \n",
       "\n",
       "   years        area  perct_irri  \n",
       "0     25  101.171570   60.802507  \n",
       "1     29  374.334800   61.901377  \n",
       "2     19  647.498030   78.435473  \n",
       "3     19  607.029392   55.098924  \n",
       "4     19  558.467041   50.088347  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = df_sites_info.site\n",
    "areas = []\n",
    "perct_irris = []\n",
    "\n",
    "for site in sites:\n",
    "    lat = float(df_sites_info[df_sites_info.site == site].lat)\n",
    "    lon = float(df_sites_info[df_sites_info.site == site].lon)\n",
    "    dist = list(enumerate(np.sqrt((lat - df_obs.lat)**2 + (lon - (df_obs.lon))**2)))\n",
    "    df_dist = pd.DataFrame(dist, columns=['rownum', 'distance'])\n",
    "    row = list(df_dist.nsmallest(5, 'distance').rownum) # select the five nearest locations and average for\n",
    "                                                        # cropping area & irrigation percentage\n",
    "    area = df_obs.iloc[row].area.mean()\n",
    "    perct_irri = df_obs.iloc[row].perct_irri.mean()\n",
    "    areas.append(area)\n",
    "    perct_irris.append(perct_irri)\n",
    "\n",
    "# add planting area & irrigation info for filtering purposes\n",
    "df_filter = pd.DataFrame({'area': areas, 'perct_irri': perct_irris})\n",
    "df_summary = pd.concat([df_sites_info, df_filter], axis=1)\n",
    "df_summary.head()\n",
    "#df_summary.to_csv('/home/disk/eos8/ach315/upscale/weadata/site_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Filter out sites with low planting and/or high irrigation\n",
    "- Main output: **siteyears_filtered**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new filtered sites are now lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior: 4225\n",
      "filtered: 2603\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "df_filtered = df_summary[(df_summary.area > 1000) & (df_summary.perct_irri < 50)] \n",
    "\n",
    "# how many site-years left?\n",
    "df_siteyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/siteyears_crithr2.csv', dtype='str', usecols=[1,2])\n",
    "siteyears_filtered = df_siteyears[df_siteyears.site.isin(df_filtered.site)]\n",
    "print('prior:', df_siteyears.shape[0])\n",
    "print('filtered:', siteyears_filtered.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compile and gap-fill usable site-years data into individual weather data files\n",
    "- Main tasks:\n",
    "    - figure out time zone for individual site and convet wea data from UTC into local time\n",
    "    - gap-fill wea data by linearly interpolating with data from hour before and after\n",
    "- Main input:\n",
    "    - /weadata/**stations_info.csv** - city, state, lat, lon info for each site\n",
    "    - /weadata/**siteyears_crithr2.csv** - site-year info for data filtered with crithr = 1\n",
    "    - df_temp, df_rh, df_precip, df_solrad\n",
    "- Main output:\n",
    "    - /weadata/data/control/**site_year.txt** - MAIZSIM weather file for every site-year\n",
    "- Functions:\n",
    "    - find_zone(site)\n",
    "    - utc_to_local(times, zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Read in weather file & site-year info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data\n",
    "df_temp = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all.csv', index_col=0)\n",
    "df_rh = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all.csv', index_col=0)\n",
    "df_precip = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all.csv', index_col=0)\n",
    "df_solrad = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all.csv', index_col=0)\n",
    "\n",
    "# site-year & filter info\n",
    "df_siteyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/siteyears_crithr2.csv', dtype='str', usecols=[1,2])\n",
    "df_summary = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_summary.csv', dtype={'site':str}, index_col=0)\n",
    "df_filtered = df_summary[(df_summary.area > 1000) & (df_summary.perct_irri < 50)] \n",
    "siteyears = df_siteyears[df_siteyears.site.isin(df_filtered.site)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Create weather file for individual site-years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 1min 9s, sys: 29.5 s, total: 1h 1min 38s\n",
      "Wall time: 1h 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set up growing season period\n",
    "season_start, season_end = '03-02', '11-30'\n",
    "\n",
    "# create individual site-year weather data\n",
    "for i in np.arange(siteyears.shape[0]):\n",
    "    # selecting site-year combinations\n",
    "    site = siteyears.iloc[i,0]\n",
    "    year = siteyears.iloc[i,1]\n",
    "#    print(site, year)\n",
    "    \n",
    "    # constructing dataframe that will hold all weather data\n",
    "    col = ['jday','date','hour','solrad','temp','precip','rh', 'co2']\n",
    "    df_wea = pd.DataFrame(columns=col)\n",
    "\n",
    "    # setting up for time-relating entries\n",
    "    times = pd.date_range(season_start + '-' + str(year), \n",
    "                          season_end + '-' + str(year)+ ' 23:00:00', freq='1H') # utc time\n",
    "    zone = find_zone(site, df_summary)\n",
    "    local_datetime = utc_to_local(times, zone)\n",
    "\n",
    "    # selecting weather data\n",
    "    utc_start, utc_end = str(times[0]), str(times[-1])\n",
    "    df_wea.temp = list(df_temp[utc_start:utc_end][site])\n",
    "    df_wea.rh = list(np.round((df_rh[utc_start:utc_end][site]), 2))\n",
    "    df_wea.precip = list(df_precip[utc_start:utc_end][site])\n",
    "    df_wea.co2 = 400    \n",
    "\n",
    "    # selecting solar radiation \n",
    "    t1 = pd.to_datetime(utc_start).to_pydatetime()\n",
    "    t2 = pd.to_datetime(utc_end).to_pydatetime()\n",
    "    tdiff = t2-t1\n",
    "    local_start = str(local_datetime[0])[:19] \n",
    "    local_end = str(pd.to_datetime(local_start).to_pydatetime() + tdiff)[:19]\n",
    "    df_wea.solrad = list(df_solrad[local_start:local_end][site]) ###*** issue here\n",
    "    \n",
    "    # adding time-relating info to data frame\n",
    "    local = pd.date_range(local_start, local_end, freq='H')\n",
    "    df_wea.jday = local.dayofyear\n",
    "    df_wea.date = local.strftime(\"'%m/%d/%Y'\")\n",
    "    df_wea.hour = local.hour    \n",
    "    \n",
    "    # gap-filling weather data\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        # creating a log file that documents the number of missing data for each site-year\n",
    "        f = open('/home/disk/eos8/ach315/upscale/weadata/data/log.txt', 'a+')\n",
    "        f.write(siteyears.iloc[i,:][0]) # site\n",
    "        f.write(', %s' %siteyears.iloc[i,:][1]) # year\n",
    "        f.write(', %s' %df_wea.isna().sum().temp) # temp\n",
    "        f.write(', %s' %df_wea.isna().sum().rh) # rh\n",
    "        f.write(', %s' %df_wea.isna().sum().precip) # precip\n",
    "        f.write(', %s\\r\\n' %df_wea.isna().sum().solrad) # solrad\n",
    "        f.close()\n",
    "        \n",
    "        # gap-filling data by linearly interpolating with data from hour before and after\n",
    "        df_wea = df_wea.interpolate() \n",
    "            \n",
    "    # saving individual site-year weather file into .csv \n",
    "    df_wea.to_csv('/home/disk/eos8/ach315/upscale/weadata/data/control/' + site + '_' + year + '.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Final step of gap-filling if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pd.interpolate() cannot gap-fill missing data if the missing data is located at the very beginning of the data (nan in first row), the code checks whether there are site-years with that situation, and if so assigns the missing data in the first row a default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob('/home/disk/eos8/ach315/upscale/weadata/data/contrl/*')\n",
    "\n",
    "for name in fnames: \n",
    "    df_wea = pd.read_csv(name)\n",
    "    df_wea = df_wea.drop(df_wea.columns[0], axis=1)\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        print(name.split('/')[-1], df_wea.isna().sum().sum())\n",
    "\n",
    "# no files required additional gap-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Read in a final compiled weather file to check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jday</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>solrad</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rh</th>\n",
       "      <th>co2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1993'</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.75</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1993'</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.62</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1993'</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.65</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1993'</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1993'</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jday          date  hour  solrad  temp  precip      rh  co2\n",
       "0    60  '03/01/1993'    18     0.0   2.8     0.0   81.75  400\n",
       "1    60  '03/01/1993'    19     0.0   1.7     0.0   81.62  400\n",
       "2    60  '03/01/1993'    20     0.0   0.6     0.0   91.65  400\n",
       "3    60  '03/01/1993'    21     0.0   0.0     0.0  100.00  400\n",
       "4    60  '03/01/1993'    22     0.0   0.0     0.0  100.00  400"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/data/control/725430_1993.txt', sep='\\t')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Compile and summarize weather data for individual site-year\n",
    "- Convert dataframe-structured weather data into single long-form list\n",
    "- Select only weather data from filtered site-years\n",
    "- Summarize data only for growing season 4/1-10/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>year</th>\n",
       "      <th>temp</th>\n",
       "      <th>rh</th>\n",
       "      <th>precip</th>\n",
       "      <th>solrad</th>\n",
       "      <th>vpd</th>\n",
       "      <th>class</th>\n",
       "      <th>station</th>\n",
       "      <th>state</th>\n",
       "      <th>tzone</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>years</th>\n",
       "      <th>area</th>\n",
       "      <th>perct_irri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>722070</td>\n",
       "      <td>1961</td>\n",
       "      <td>22.527191</td>\n",
       "      <td>74.713304</td>\n",
       "      <td>881.8</td>\n",
       "      <td>234.060642</td>\n",
       "      <td>0.701216</td>\n",
       "      <td>1</td>\n",
       "      <td>SAVANNAH INTL AP</td>\n",
       "      <td>GA</td>\n",
       "      <td>-5</td>\n",
       "      <td>32.117</td>\n",
       "      <td>-81.2</td>\n",
       "      <td>22</td>\n",
       "      <td>2445.923794</td>\n",
       "      <td>6.585904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>722070</td>\n",
       "      <td>1962</td>\n",
       "      <td>23.289261</td>\n",
       "      <td>75.118104</td>\n",
       "      <td>973.0</td>\n",
       "      <td>233.714984</td>\n",
       "      <td>0.723302</td>\n",
       "      <td>1</td>\n",
       "      <td>SAVANNAH INTL AP</td>\n",
       "      <td>GA</td>\n",
       "      <td>-5</td>\n",
       "      <td>32.117</td>\n",
       "      <td>-81.2</td>\n",
       "      <td>22</td>\n",
       "      <td>2445.923794</td>\n",
       "      <td>6.585904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>722070</td>\n",
       "      <td>1965</td>\n",
       "      <td>23.004871</td>\n",
       "      <td>75.056250</td>\n",
       "      <td>226.6</td>\n",
       "      <td>229.640454</td>\n",
       "      <td>0.712474</td>\n",
       "      <td>1</td>\n",
       "      <td>SAVANNAH INTL AP</td>\n",
       "      <td>GA</td>\n",
       "      <td>-5</td>\n",
       "      <td>32.117</td>\n",
       "      <td>-81.2</td>\n",
       "      <td>22</td>\n",
       "      <td>2445.923794</td>\n",
       "      <td>6.585904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>722070</td>\n",
       "      <td>1970</td>\n",
       "      <td>23.873122</td>\n",
       "      <td>72.770417</td>\n",
       "      <td>275.7</td>\n",
       "      <td>228.436228</td>\n",
       "      <td>0.820531</td>\n",
       "      <td>1</td>\n",
       "      <td>SAVANNAH INTL AP</td>\n",
       "      <td>GA</td>\n",
       "      <td>-5</td>\n",
       "      <td>32.117</td>\n",
       "      <td>-81.2</td>\n",
       "      <td>22</td>\n",
       "      <td>2445.923794</td>\n",
       "      <td>6.585904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>722070</td>\n",
       "      <td>1971</td>\n",
       "      <td>23.547535</td>\n",
       "      <td>72.725135</td>\n",
       "      <td>564.4</td>\n",
       "      <td>227.596440</td>\n",
       "      <td>0.805592</td>\n",
       "      <td>1</td>\n",
       "      <td>SAVANNAH INTL AP</td>\n",
       "      <td>GA</td>\n",
       "      <td>-5</td>\n",
       "      <td>32.117</td>\n",
       "      <td>-81.2</td>\n",
       "      <td>22</td>\n",
       "      <td>2445.923794</td>\n",
       "      <td>6.585904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     site  year       temp         rh  precip      solrad       vpd  class  \\\n",
       "0  722070  1961  22.527191  74.713304   881.8  234.060642  0.701216      1   \n",
       "1  722070  1962  23.289261  75.118104   973.0  233.714984  0.723302      1   \n",
       "2  722070  1965  23.004871  75.056250   226.6  229.640454  0.712474      1   \n",
       "3  722070  1970  23.873122  72.770417   275.7  228.436228  0.820531      1   \n",
       "4  722070  1971  23.547535  72.725135   564.4  227.596440  0.805592      1   \n",
       "\n",
       "            station state  tzone     lat   lon  years         area  perct_irri  \n",
       "0  SAVANNAH INTL AP    GA     -5  32.117 -81.2     22  2445.923794    6.585904  \n",
       "1  SAVANNAH INTL AP    GA     -5  32.117 -81.2     22  2445.923794    6.585904  \n",
       "2  SAVANNAH INTL AP    GA     -5  32.117 -81.2     22  2445.923794    6.585904  \n",
       "3  SAVANNAH INTL AP    GA     -5  32.117 -81.2     22  2445.923794    6.585904  \n",
       "4  SAVANNAH INTL AP    GA     -5  32.117 -81.2     22  2445.923794    6.585904  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_all = [np.nan]*siteyears.shape[0]\n",
    "rh_all = [np.nan]*siteyears.shape[0]\n",
    "precip_all = [np.nan]*siteyears.shape[0]\n",
    "solrad_all = [np.nan]*siteyears.shape[0]\n",
    "\n",
    "for i in np.arange(siteyears.shape[0]):\n",
    "    # growing season temp mean for each site-year\n",
    "    temp = df_temp.loc[siteyears.iloc[i,1] + '-04-01' : \n",
    "                       siteyears.iloc[i,1] +'-10-31', \n",
    "                       siteyears.iloc[i,0]].mean()\n",
    "    # growing season RH mean for each site-year\n",
    "    rh = df_rh.loc[siteyears.iloc[i,1] + '-04-01' : \n",
    "                   siteyears.iloc[i,1] +'-10-31', \n",
    "                   siteyears.iloc[i,0]].mean()\n",
    "    # growing season precip sum for each site-year\n",
    "    precip = df_precip.loc[siteyears.iloc[i,1] + '-04-01' : \n",
    "                           siteyears.iloc[i,1] +'-10-31', \n",
    "                           siteyears.iloc[i,0]].sum()\n",
    "\n",
    "    # \n",
    "    solrad = df_solrad.loc[siteyears.iloc[i,1] + '-04-01' : \n",
    "                           siteyears.iloc[i,1] +'-10-31', \n",
    "                           siteyears.iloc[i,0]].mean()\n",
    "    \n",
    "    \n",
    "    temp_all[i] = temp\n",
    "    rh_all[i] = rh\n",
    "    precip_all[i] = precip\n",
    "    solrad_all[i] = solrad\n",
    "    \n",
    "# calculating VPD based on temperature & RH\n",
    "vpd_all = []\n",
    "for i in np.arange(len(temp_all)):\n",
    "    vpd_all.append(CC_VPD(temp_all[i], rh_all[i]/100))\n",
    "    \n",
    "# storing output in dataframe\n",
    "df_siteyears_weamean = siteyears.copy()\n",
    "df_siteyears_weamean['temp'] = list(temp_all)\n",
    "df_siteyears_weamean['rh'] = list(rh_all)\n",
    "df_siteyears_weamean['precip'] = list(precip_all)\n",
    "df_siteyears_weamean['solrad'] = list(solrad_all)\n",
    "df_siteyears_weamean['vpd'] = list(vpd_all)\n",
    "df_siteyears_weamean = pd.merge(df_siteyears_weamean, df_filtered, on='site')\n",
    "\n",
    "df_siteyears_weamean.head()\n",
    "#df_siteyears_weamean.to_csv('/home/disk/eos8/ach315/upscale/weadata/wea_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ideotype]",
   "language": "python",
   "name": "conda-env-ideotype-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
