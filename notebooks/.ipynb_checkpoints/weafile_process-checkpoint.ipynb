{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather file - gap filling and formatting\n",
    "- Main tasks: \n",
    "    1. gap fill weather file\n",
    "    2. combine met and solrad info into single files for each site-year\n",
    "    3. address timezone issue\n",
    "    4. format weather file into MAIZSIM-readable format\n",
    "- Data source: \n",
    "    1. weadata/**temp_all.csv**\n",
    "    2. weadata/**rh_all.csv**\n",
    "    3. weadata/**precip_all.csv**\n",
    "    4. weadata/**solrad_all.csv**\n",
    "- Main output: \n",
    "    - weadata/data/control/**site_year.txt** - weather file for all site-years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import time \n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.io.shapereader as shpreader\n",
    "\n",
    "from palettable.colorbrewer.sequential import OrRd_6\n",
    "from palettable.colorbrewer.sequential import YlGn_9\n",
    "from palettable.colorbrewer.sequential import YlGnBu_8\n",
    "from palettable.colorbrewer.sequential import RdPu_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Reading in temperature, precip, RH & solar radiation data:\n",
    "- Main input:\n",
    "    - /weadata/**temp_all.csv**\n",
    "    - /weadata/**rh_all.csv**\n",
    "    - /weadata/**precip_all.csv**\n",
    "    - /weadata/**solrad_all.csv**\n",
    "- Main output: \n",
    "    - **df_temp, df_rh, df_precip, df_solrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 Read in weather data 1961-1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198000, 237)\n",
      "(198000, 237)\n",
      "(198000, 237)\n",
      "(198000, 237)\n"
     ]
    }
   ],
   "source": [
    "# read in individual weather data\n",
    "df_temp_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/temp_6190.csv', index_col=0)\n",
    "df_rh_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/rh_6190.csv', index_col=0)\n",
    "df_precip_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/precip_6190.csv', index_col=0)\n",
    "df_solrad_6190 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_6190.csv', index_col=0)\n",
    "\n",
    "# re-index solar radiation data to only include growing season\n",
    "df_solrad_6190 = df_solrad_6190.reindex(df_temp_6190.index)\n",
    "\n",
    "# check that all met elements aligned - dataframe shape should match\n",
    "print(df_temp_6190.shape)\n",
    "print(df_rh_6190.shape)\n",
    "print(df_precip_6190.shape)\n",
    "print(df_solrad_6190.shape)\n",
    "\n",
    "# convert station ID header from WBAN to USAF (in order to make continuous with 1991-2010)\n",
    "df_stations = pd.read_csv('/home/disk/eos8/ach315/data/ISH_NSRD/stations_wban_usaf.csv', header=None, dtype='str')\n",
    "df_stations.columns = ['WBAN', 'USAF']\n",
    "sites_wban = list(df_temp_6190.columns)\n",
    "sites_usaf = df_stations[df_stations['WBAN'].isin(sites_wban)]['USAF']\n",
    "\n",
    "# assign new USAF headers\n",
    "df_temp_6190.columns = sites_usaf; df_temp_6190 = df_temp_6190.sort_index(axis=1)\n",
    "df_rh_6190.columns = sites_usaf; df_rh_6190 = df_rh_6190.sort_index(axis=1)\n",
    "df_precip_6190.columns = sites_usaf; df_precip_6190 = df_precip_6190.sort_index(axis=1)\n",
    "df_solrad_6190.columns = sites_usaf; df_solrad_6190 = df_solrad_6190.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2 Read in weather data 1991-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132000, 241)\n",
      "(132000, 241)\n",
      "(132000, 241)\n",
      "(132000, 242)\n"
     ]
    }
   ],
   "source": [
    "## read in individual weather data\n",
    "df_temp_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/temp_9110_class1.csv', index_col=0)\n",
    "df_rh_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/rh_9110_class1.csv', index_col=0)\n",
    "df_precip_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/precip_9110_class1.csv', index_col=0)\n",
    "df_solrad_9110 = pd.read_csv( '/home/disk/eos8/ach315/upscale/weadata/solrad_9110_class1.csv', index_col=0)\n",
    "\n",
    "# re-index solar radiation data to only include growing season\n",
    "df_solrad_9110 = df_solrad_9110.reindex(df_temp_9110.index)\n",
    "\n",
    "# check that all met elements aligned - dataframe shape should match\n",
    "print(df_temp_9110.shape)\n",
    "print(df_rh_9110.shape)\n",
    "print(df_precip_9110.shape)\n",
    "print(df_solrad_9110.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.3 Stitch together weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330000, 274)\n",
      "(330000, 274)\n",
      "(330000, 274)\n",
      "(330000, 274)\n"
     ]
    }
   ],
   "source": [
    "df_temp = pd.concat([df_temp_6190, df_temp_9110], axis=0, join='outer'); df_temp = df_temp.sort_index(axis=1)\n",
    "df_rh = pd.concat([df_rh_6190, df_rh_9110], axis=0, join='outer'); df_rh = df_rh.sort_index(axis=1)\n",
    "df_precip = pd.concat([df_precip_6190, df_precip_9110], axis=0, join='outer'); df_precip = df_precip.sort_index(axis=1)\n",
    "df_solrad = pd.concat([df_solrad_6190, df_solrad_9110], axis=0, join='outer'); df_solrad = df_solrad.sort_index(axis=1)\n",
    "\n",
    "print(df_temp.shape)\n",
    "print(df_rh.shape)\n",
    "print(df_precip.shape)\n",
    "print(df_solrad.shape)\n",
    "\n",
    "#df_temp.to_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all.csv')\n",
    "#df_rh.to_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all.csv')\n",
    "#df_precip.to_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all.csv')\n",
    "#df_solrad.to_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_temp.to_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all_test.csv')\n",
    "#df_rh.to_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all_test.csv')\n",
    "#df_precip.to_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all_test.csv')\n",
    "#df_solrad.to_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Figure out valid site-years that can be gap-filled\n",
    "Selecting for site-years based on **crit_hrs** - consecutive missing hours of datapoints within raw data\n",
    "- Main input: **df_temp, df_precip, df_solrad**\n",
    "- Main output: **finalist**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 32s, sys: 89 ms, total: 3min 32s\n",
      "Wall time: 3min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# input variables for loop\n",
    "datasets = list([df_temp, df_precip, df_solrad]) # weather datasets to process\n",
    "                                                 # df_rh is based off df_temp, so no need to evaluate \n",
    "\n",
    "finalist = list([[], [], []]) # final lists to store processed output\n",
    "                              # order: [0]-temp, [1]-precip, [2]-solrad\n",
    "\n",
    "years = np.arange(1961, 2011) # years\n",
    "growseason_start = '-03-01 00:00:00'\n",
    "growseason_end = '-11-30 23:00:00' \n",
    "\n",
    "crit_hrs = 2 # critical hrs of missing data\n",
    "\n",
    "# loop through temp, precip & solrad dataset to pick out usable site-years\n",
    "for i in np.arange(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    siteyears_all = list()\n",
    "    sites = dataset.columns\n",
    "    \n",
    "    for j in years:\n",
    "        start_time = str(j) + growseason_start\n",
    "        end_time = str(j) + growseason_end\n",
    "        siteyears = list()\n",
    "        \n",
    "        for k in sites:\n",
    "            df = dataset.loc[start_time:end_time, k] \n",
    "            df = pd.DataFrame(df)\n",
    "            df['group'] = df.notnull().astype(int) # df.notnull() returns TRUE or FALSE, \n",
    "                                                   # .astype(int) turns TRUE into 1, and FALSE into 0\n",
    "            df['group'] = df.group.cumsum() # calculating cumulative sum \n",
    "            df = df[df.iloc[:,0].isnull()] # selecting out individual timesteps that have missing data\n",
    "            nans_list = df.groupby('group')['group'].count() # counts the number of consecutive NANs \n",
    "            if nans_list[nans_list > crit_hrs].shape[0] == 0:\n",
    "                use_siteyear = str(j) + '_' + str(k)\n",
    "                siteyears.append(use_siteyear) # only record site-years that have fewer consecutive NANs than the critical value set\n",
    "\n",
    "            # The logic of this section of code:\n",
    "            # If weadata is absent (df.notnull == FALSE) you get a return of 0, thus,\n",
    "            # df.group.cumsum() would not change the cumulative sum when encountering NANs since you're only adding 0.\n",
    "            # By doing so, you end up with repeated cumsum() values when you have multiple NANs following it.\n",
    "            # cumsum() values are documented in the 'group' column.\n",
    "            # groupby('group') allows you to then group the cumsum() values into groups and document their counts. \n",
    "            # If a specific cumsum() values has counts greater than 1, that means there were NAN values that followed it.\n",
    "            # The code then evaluates whether there were consecutive NAN values that exceeded the designated critical values.\n",
    "            # If so, that site-years is excluded. \n",
    "        \n",
    "        siteyears_all.extend(siteyears)\n",
    "    \n",
    "    finalist[i] = siteyears_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Compare usable site-years for temp  & precip and find the common year-sites\n",
    "- Main intput: **finalist**\n",
    "- Main output: **siteyears**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 2331\n",
      "precip: 1854\n",
      "solrad: 11937\n",
      "overlapping siteyears: 1673\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 0\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr0 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 4298\n",
      "precip: 3154\n",
      "solrad: 11937\n",
      "overlapping siteyears: 3021\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 1\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr1 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 6096\n",
      "precip: 4397\n",
      "solrad: 11937\n",
      "overlapping siteyears: 4225\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 2\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr2 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 6254\n",
      "precip: 4564\n",
      "solrad: 11937\n",
      "overlapping siteyears: 4398\n"
     ]
    }
   ],
   "source": [
    "# assign output to individual siteyears - crithr = 3\n",
    "siteyears_temp = finalist[0]\n",
    "siteyears_precip = finalist[1]\n",
    "siteyears_solrad = finalist[2]\n",
    "print('temp:', len(siteyears_temp))\n",
    "print('precip:', len(siteyears_precip))\n",
    "print('solrad:', len(siteyears_solrad))\n",
    "\n",
    "# identify overlapping siteyears\n",
    "siteyears = list(set(siteyears_temp) & set(siteyears_precip))\n",
    "siteyears = list(set(siteyears) & set(siteyears_solrad))\n",
    "siteyears.sort()\n",
    "siteyears_crithr3 = siteyears\n",
    "\n",
    "print('overlapping siteyears:', len(siteyears))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Store basic info of valid site-years\n",
    "- Main input: **siteyears_crithr2**\n",
    "    - crithr2 seems to be the best interval given the balance between gaining siteyears vs. limiting gap filling\n",
    "- Main output: \n",
    "    - weadata/**siteyears_crithr2.csv** - site-year info for data filtered with crithr = 2\n",
    "    - weadata/**site_nyears_crithr2.csv** - info on how many years of wea data each site has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siteyears = siteyears_crithr2 ### update this\n",
    "\n",
    "# what are the valid site-years?\n",
    "years = list()\n",
    "sites = list()\n",
    "\n",
    "for i in range(len(siteyears)):\n",
    "    year = siteyears[i][0:4]\n",
    "    years.append(year)\n",
    "    site = siteyears[i][5:11] \n",
    "    sites.append(site)\n",
    "\n",
    "df_siteyears = pd.DataFrame({'site': sites, 'year': years}, \n",
    "                            columns=['site', 'year'])\n",
    "df_siteyears = df_siteyears.sort_values(['site', 'year'])\n",
    "final_sites = list(set(df_siteyears.site))\n",
    "\n",
    "# how many years of data do each site have?\n",
    "site_nyears = list()\n",
    "\n",
    "for i in final_sites:\n",
    "    years = len(df_siteyears[df_siteyears[\"site\"] == i])\n",
    "    site_nyears.append(years)\n",
    "    \n",
    "df_site_nyears = pd.DataFrame({\"site\": final_sites, \"years\": site_nyears})\n",
    "df_site_nyears = df_site_nyears.sort_values([\"site\"])\n",
    "df_site_nyears = df_site_nyears.reset_index().iloc[:, 1:3]\n",
    "\n",
    "# writing out info as .csv\n",
    "#df_siteyears.to_csv('../weadata/siteyears_crithr2.csv')\n",
    "#df_site_nyears.to_csv(\"../weadata/site_nyears_crithr2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter sites based on planting area & irrigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Select sites with valid weather data\n",
    "- Main output: \n",
    "    - **df_sites_info**\n",
    "    - **wea_summary.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in station & site-year info\n",
    "df_site_nyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_nyears_crithr2.csv', \n",
    "                             index_col=0, dtype={'site': str})\n",
    "df_stations_9110 = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/stations_info_9110.csv', \n",
    "                               dtype={'USAF': str}, usecols=[0,1,3,4,8,9,10])\n",
    "df_sites_info = df_stations_9110[df_stations_9110.USAF.isin(df_site_nyears.site)]\n",
    "df_sites_info.columns = ['site', 'class', 'station', 'state', 'tzone', 'lat', 'lon']\n",
    "\n",
    "# merge site info & site-years info\n",
    "df_sites_info = pd.merge(df_sites_info, df_site_nyears, on='site')\n",
    "\n",
    "# drop stations from Alaska, Guam, Hawaii & Puerto Rico\n",
    "df_sites_info = df_sites_info[(df_sites_info.state != 'AK') & (df_sites_info.state != 'GU') & \n",
    "                              (df_sites_info.state != 'HI')& (df_sites_info.state != 'PR')]\n",
    "\n",
    "# final station list\n",
    "df_sites_info.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Include planting area & irrigation info\n",
    "- Main output: \n",
    "    - df_obs: obs_areairri.csv\n",
    "    - df_summary: wea_summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/stateID.txt', header=None, sep='\\s+')\n",
    "county = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/countyID.txt', header=None, sep='\\s+')\n",
    "lat = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/lat_county.txt', header=None, sep='\\s+')\n",
    "lon = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/lon_county.txt', header=None, sep='\\s+')\n",
    "irri = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/irr_area_acres.txt', header=None, sep='\\s+')\n",
    "area = pd.read_csv('/home/disk/eos3/aswann/Shared/Data/irrigated_area/crop_area_acres.txt', header=None, sep='\\s+')\n",
    "state = state.iloc[0,:]\n",
    "county = county.iloc[0,:]\n",
    "lat = lat.iloc[0,:]\n",
    "lon = lon.iloc[0,:]\n",
    "\n",
    "# raw data includes data from 4 censuses that show data of 1997, 2002, 2007 & 2012\n",
    "# we average data from all 4 censuses \n",
    "irri = irri.mean(axis=1)\n",
    "area = area.mean(axis=1)\n",
    "df_obs = pd.DataFrame({'state': state, 'county': county, 'lat': lat, 'lon': lon, \n",
    "                       'irri_area': irri, 'crop_area': area, 'perct_irri': irri/area*100})\n",
    "#df_obs.to_csv('/home/disk/eos8/ach315/upscale/weadata/obs_areairri.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = df_sites_info.site\n",
    "areas = []\n",
    "perct_irris = []\n",
    "\n",
    "for site in sites:\n",
    "    lat = float(df_sites_info[df_sites_info.site == site].lat)\n",
    "    lon = float(df_sites_info[df_sites_info.site == site].lon)\n",
    "    dist = list(enumerate(np.sqrt((lat - df_obs.lat)**2 + (lon - (df_obs.lon))**2)))\n",
    "    df_dist = pd.DataFrame(dist, columns=['rownum', 'distance'])\n",
    "    row = list(df_dist.nsmallest(5, 'distance').rownum) # select the five nearest locations and average for\n",
    "                                                        # cropping area & irrigation percentage\n",
    "    area = df_obs.iloc[row].crop_area.mean()\n",
    "    perct_irri = df_obs.iloc[row].perct_irri.mean()\n",
    "    areas.append(area)\n",
    "    perct_irris.append(perct_irri)\n",
    "\n",
    "# add planting area & irrigation info for filtering purposes\n",
    "df_filter = pd.DataFrame({'area': areas, 'perct_irri': perct_irris})\n",
    "df_summary = pd.concat([df_sites_info, df_filter], axis=1)\n",
    "df_summary.head()\n",
    "#df_summary.to_csv('/home/disk/eos8/ach315/upscale/weadata/site_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Filter out sites with low planting and/or high irrigation\n",
    "- Main output: **siteyears_filtered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior: 4225\n",
      "filtered: 3300\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "df_filtered = df_summary[(df_summary.area > 1000) & (df_summary.perct_irri < 50)] \n",
    "\n",
    "# how many site-years left?\n",
    "df_siteyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/siteyears_crithr2.csv', dtype='str', usecols=[1,2])\n",
    "siteyears_filtered = df_siteyears[df_siteyears.site.isin(df_filtered.site)]\n",
    "print('prior:', df_siteyears.shape[0])\n",
    "print('filtered:', siteyears_filtered.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compile and gap-fill usable site-years data into individual weather data files\n",
    "- Main tasks:\n",
    "    - figure out time zone for individual site and convet wea data from UTC into local time\n",
    "    - gap-fill wea data by linearly interpolating with data from hour before and after\n",
    "- Main input:\n",
    "    - /weadata/**stations_info.csv** - city, state, lat, lon info for each site\n",
    "    - /weadata/**siteyears_crithr1.csv** - site-year info for data filtered with crithr = 1\n",
    "    - df_temp, df_rh, df_precip, df_solrad\n",
    "- Main output:\n",
    "    - /weadata/data/control/**site_year.txt** - MAIZSIM weather file for every site-year\n",
    "- Functions:\n",
    "    - find_zone(site)\n",
    "    - utc_to_local(times, zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Setting up functions to handle time zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time zone finder\n",
    "def find_zone(site):\n",
    "    \"\"\"\n",
    "    find time zone for specific sites\n",
    "    \"\"\"\n",
    "    lat = float(siteinfo[siteinfo.site == site].lat)\n",
    "    lon = float(siteinfo[siteinfo.site == site].lon)\n",
    "    tf = TimezoneFinder()    \n",
    "    zone = tf.timezone_at(lng=lon, lat=lat)\n",
    "    return zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utc conversion to local time\n",
    "def utc_to_local(times, zone):\n",
    "    \"\"\"\n",
    "    convert list of utc timestamps into local time\n",
    "    \"\"\"\n",
    "    times = times.to_pydatetime() # convert from pd.DatetimeIndex into python datetime format\n",
    "    utc = pytz.timezone('UTC') # setting up the UTC timezone, requires package 'pytz'\n",
    "    local_datetime = list()\n",
    "    \n",
    "    for time in times:\n",
    "        utctime = utc.localize(time) # adding UTC timezone to datetime\n",
    "        localtime = utctime.astimezone(pytz.timezone(zone)) \n",
    "        datetime = pd.to_datetime(localtime)\n",
    "        local_datetime.append(datetime)\n",
    "        \n",
    "    return local_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Reading in all weather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in weather data\n",
    "#df_temp = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all.csv', index_col=0)\n",
    "#df_rh = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all.csv', index_col=0)\n",
    "#df_precip = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all.csv', index_col=0)\n",
    "#df_solrad = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Creating weather file for individual site-years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 42min 4s, sys: 38.4 s, total: 1h 42min 43s\n",
      "Wall time: 1h 41min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read in additional files\n",
    "df_siteyears = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/siteyears_crithr2.csv', dtype='str', usecols=[1,2])\n",
    "df_filtered = df_summary[(df_summary.area > 1000) & (df_summary.perct_irri < 50)] \n",
    "siteyears = df_siteyears[df_siteyears.site.isin(df_filtered.site)]\n",
    "\n",
    "# set up growing season period\n",
    "season_start, season_end = '03-02', '11-30'\n",
    "\n",
    "# create individual site-year weather data\n",
    "for i in np.arange(siteyears.shape[0]):\n",
    "    # selecting site-year combinations\n",
    "    site = siteyears.iloc[i,0]\n",
    "    year = siteyears.iloc[i,1]\n",
    "#    print(site, year)\n",
    "    \n",
    "    # constructing dataframe that will hold all weather data\n",
    "    col = ['jday','date','hour','solrad','temp','precip','rh', 'co2']\n",
    "    df_wea = pd.DataFrame(columns=col)\n",
    "\n",
    "    # setting up for time-relating entries\n",
    "    times = pd.date_range(season_start + '-' + str(year), \n",
    "                          season_end + '-' + str(year)+ ' 23:00:00', freq='1H') # utc time\n",
    "    zone = find_zone(site)\n",
    "    local_datetime = utc_to_local(times, zone)\n",
    "\n",
    "    # selecting weather data\n",
    "    utc_start, utc_end = str(times[0]), str(times[-1])\n",
    "    df_wea.temp = list(df_temp[utc_start:utc_end][site])\n",
    "    df_wea.rh = list(np.round((df_rh[utc_start:utc_end][site]), 2))\n",
    "    df_wea.precip = list(df_precip[utc_start:utc_end][site])\n",
    "    df_wea.co2 = 400    \n",
    "\n",
    "    # selecting solar radiation \n",
    "    t1 = pd.to_datetime(utc_start).to_pydatetime()\n",
    "    t2 = pd.to_datetime(utc_end).to_pydatetime()\n",
    "    tdiff = t2-t1\n",
    "    local_start = str(local_datetime[0])[:19] \n",
    "    local_end = str(pd.to_datetime(local_start).to_pydatetime() + tdiff)[:19]\n",
    "    df_wea.solrad = list(df_solrad[local_start:local_end][site]) ###*** issue here\n",
    "    \n",
    "    # adding time-relating info to data frame\n",
    "    local = pd.date_range(local_start, local_end, freq='H')\n",
    "    df_wea.jday = local.dayofyear\n",
    "    df_wea.date = local.strftime(\"'%m/%d/%Y'\")\n",
    "    df_wea.hour = local.hour    \n",
    "    \n",
    "    # gap-filling weather data\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        # creating a log file that documents the number of missing data for each site-year\n",
    "        f = open('/home/disk/eos8/ach315/upscale/weadata/data/log.txt', 'a+')\n",
    "        f.write(siteyears.iloc[i,:][0]) # site\n",
    "        f.write(', %s' %siteyears.iloc[i,:][1]) # year\n",
    "        f.write(', %s' %df_wea.isna().sum().temp) # temp\n",
    "        f.write(', %s' %df_wea.isna().sum().rh) # rh\n",
    "        f.write(', %s' %df_wea.isna().sum().precip) # precip\n",
    "        f.write(', %s\\r\\n' %df_wea.isna().sum().solrad) # solrad\n",
    "        f.close()\n",
    "        \n",
    "        # gap-filling data by linearly interpolating with data from hour before and after\n",
    "        df_wea = df_wea.interpolate() \n",
    "            \n",
    "    # saving individual site-year weather file into .csv \n",
    "#    df_wea.to_csv('/home/disk/eos8/ach315/upscale/weadata/data/control/' + site + '_' + year + '.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Final step of gap-filling if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pd.interpolate() cannot gap-fill missing data if the missing data is located at the very beginning of the data (nan in first row), the code checks whether there are site-years with that situation, and if so assigns the missing data in the first row a default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob('/home/disk/eos8/ach315/upscale/weadata/data/test/*')\n",
    "\n",
    "for name in fnames: \n",
    "    df_wea = pd.read_csv(name)\n",
    "    df_wea = df_wea.drop(df_wea.columns[0], axis=1)\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        print(name.split('/')[-1], df_wea.isna().sum().sum())\n",
    "\n",
    "# no files required additional gap-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: reading in a final compiled weather file to check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jday</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>solrad</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rh</th>\n",
       "      <th>co2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1979'</td>\n",
       "      <td>14</td>\n",
       "      <td>319.0</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.17</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1979'</td>\n",
       "      <td>15</td>\n",
       "      <td>266.0</td>\n",
       "      <td>-12.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.70</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1979'</td>\n",
       "      <td>16</td>\n",
       "      <td>185.0</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.40</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1979'</td>\n",
       "      <td>17</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.17</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>'03/01/1979'</td>\n",
       "      <td>18</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-14.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.24</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jday          date  hour  solrad  temp  precip     rh  co2\n",
       "0    60  '03/01/1979'    14   319.0 -13.3     0.0  41.17  400\n",
       "1    60  '03/01/1979'    15   266.0 -12.7     0.0  41.70  400\n",
       "2    60  '03/01/1979'    16   185.0 -13.2     0.0  43.40  400\n",
       "3    60  '03/01/1979'    17    87.0 -13.3     0.0  41.17  400\n",
       "4    60  '03/01/1979'    18    21.0 -14.9     0.0  45.24  400"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/data/test/702310_1979.txt', sep='\\t')\n",
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ideotype]",
   "language": "python",
   "name": "conda-env-ideotype-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
