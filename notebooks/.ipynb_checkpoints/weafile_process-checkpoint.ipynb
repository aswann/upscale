{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying & Quantifying Missing Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import time \n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in temperature, precip, RH & solar radiation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262968, 237)\n",
      "(262968, 237)\n",
      "(262968, 237)\n",
      "(262968, 237)\n"
     ]
    }
   ],
   "source": [
    "# these are loading the updated files generated through a more streamlined process\n",
    "df_temp = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/temp_all.csv', index_col= 0)\n",
    "df_rh = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/rh_all.csv', index_col= 0)\n",
    "df_precip = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/precip_all.csv', index_col= 0)\n",
    "df_solrad = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/backup/solrad_all.csv', index_col= 0)\n",
    "\n",
    "# processing solar radiation data to only include growing season\n",
    "df_solrad = df_solrad.reindex(df_temp.index)\n",
    "print(df_temp.shape)\n",
    "print(df_rh.shape)\n",
    "print(df_precip.shape)\n",
    "print(df_solrad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the updated solar radiation data\n",
    "df_solrad.to_csv('/home/disk/eos8/ach315/upscale/weadata/solrad_all.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all the weather data into xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (loc: 237, time: 262968)\n",
       "Coordinates:\n",
       "  * time           (time) datetime64[ns] 1961-01-01 1961-01-01T01:00:00 ...\n",
       "  * loc            (loc) object '03103' '03812' '03813' '03820' '03822' ...\n",
       "Data variables:\n",
       "    temperature    (time, loc) float64 nan 3.3 14.4 9.4 11.7 9.4 nan nan 6.1 ...\n",
       "    precipitation  (time, loc) float64 nan nan 109.0 48.0 0.0 13.0 nan nan ...\n",
       "    relhumidity    (time, loc) float64 nan 0.892 0.9677 0.9665 0.9288 0.9665 ...\n",
       "    solrad         (time, loc) float64 nan 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ..."
      ]
     },
     "execution_count": 1366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timepoints = pd.to_datetime(df_temp.index)\n",
    "sites = df_temp.columns\n",
    "\n",
    "ds_weadata = xr.Dataset({\"temperature\": ([\"time\", \"loc\"], df_temp),\n",
    "                         \"precipitation\": ([\"time\", \"loc\"], df_precip),\n",
    "                         \"relhumidity\": ([\"time\", \"loc\"], df_rh),\n",
    "                         \"solrad\": ([\"time\", \"loc\"], df_solrad)},\n",
    "                        coords= {\"time\": timepoints,\n",
    "                                 \"loc\": sites})\n",
    "\n",
    "ds_weadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JH 190910: *** Depricated Code ***\n",
    "# these are loading the old weather files that are now in backup\n",
    "df_temp = pd.read_csv(\"/home/disk/eos8/ach315/upscale/weadata/backup/temp_all.csv\", index_col= 0)\n",
    "df_precip = pd.read_csv(\"/home/disk/eos8/ach315/upscale/weadata/backup/precip_all.csv\", index_col= 0)\n",
    "df_rh = pd.read_csv(\"/home/disk/eos8/ach315/upscale/weadata/backup/rh_all.csv\", index_col= 0)\n",
    "df_solrad = pd.read_csv(\"/home/disk/eos8/ach315/upscale/weadata/backup/solrad_all.csv\", index_col= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Figure out site-years that can be gap-filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Setting up variables\n",
    "# variable input for loop\n",
    "\n",
    "# weather datasets to process\n",
    "datasets = list([df_temp, df_rh, df_precip, df_solrad]) # df_rh is based off df_temp, so no need to evaluate \n",
    "\n",
    "# final lists to store processed output\n",
    "finalist = list([[], [], [], []]) # order: [0]-temp, [1]-precip, [2]-solrad\n",
    "\n",
    "# years\n",
    "years = np.arange(1961, 1991)\n",
    "\n",
    "# growing season\n",
    "growseason_start = '-04-01 00:00:00' # *** maybe want to change this to earlier, -03-01 00:00:00\n",
    "growseason_end = '-10-31 23:00:00'\n",
    "\n",
    "# critical hrs of missing data\n",
    "crit_hrs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Loop\n",
    "# looking through the temp, precip & solrad dataset to pick out usable site-years\n",
    "\n",
    "for i in np.arange(len(datasets)):\n",
    "    dataset = datasets[i]\n",
    "    siteyears_all = list()\n",
    "    sites = dataset.columns\n",
    "    \n",
    "    for j in years:\n",
    "        start_time = str(j) + growseason_start\n",
    "        end_time = str(j) + growseason_end\n",
    "        siteyears = list()\n",
    "        \n",
    "        for k in sites:\n",
    "            df = dataset.loc[start_time:end_time, k] \n",
    "            df = pd.DataFrame(df)\n",
    "            df['group'] = df.notnull().astype(int) # df.notnull() returns TRUE or FALSE, \n",
    "                                                   # .astype(int) turns TRUE into 1, and FALSE into 0\n",
    "            df['group'] = df.group.cumsum() # calculating cumulative sum \n",
    "            df = df[df.iloc[:,0].isnull()] # selecting out individual timesteps that have missing data\n",
    "            df['count'] = df.groupby('group')['group'].transform('size') # counts the number of consecutive NANs \n",
    "            df = df.drop_duplicates('group')\n",
    "            \n",
    "            if df[df['count'] > crit_hrs].shape[0] == 0:\n",
    "                use_siteyear = str(j) + '_' + str(k)\n",
    "                siteyears.append(use_siteyear) # only record site-years that have fewer consecutive NANs than the critical value set\n",
    "        \n",
    "        siteyears_all.extend(siteyears)\n",
    "    \n",
    "    finalist[i] = siteyears_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Compare usable site-years for temp  & precip and find the common year-sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: 2838\n",
      "precip: 2254\n",
      "solrad: 7097\n",
      "overlapping siteyears: 2125\n"
     ]
    }
   ],
   "source": [
    "yearsites_temp = finalist[0]\n",
    "yearsites_rh = finalist[1]\n",
    "yearsites_precip = finalist[2]\n",
    "yearsites_solrad = finalist[3]\n",
    "\n",
    "print('temp:', len(yearsites_temp))\n",
    "print('precip:', len(yearsites_precip))\n",
    "print('solrad:', len(yearsites_solrad))\n",
    "\n",
    "yearsites = list(set(yearsites_temp) & set(yearsites_rh))\n",
    "yearsites = list(set(yearsites) & set(yearsites_precip))\n",
    "yearsites = list(set(yearsites) & set(yearsites_solrad))\n",
    "\n",
    "yearsites.sort()\n",
    "\n",
    "print('overlapping siteyears:', len(yearsites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crithrs_0 = len(yearsites)\n",
    "#crithrs_1 = len(yearsites)\n",
    "#crithrs_2 = len(yearsites)\n",
    "#crithrs_3 = len(yearsites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0,1,2,3]\n",
    "width =  [crithrs_0, crithrs_1, crithrs_2, crithrs_3]\n",
    "plt.bar(x, width, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Parse out the year-sites into a dataframe that has usable year as a column & site as a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list()\n",
    "sites = list()\n",
    "\n",
    "for i in range(len(yearsites)):\n",
    "    year = yearsites[i][0:4]\n",
    "    years.append(year)\n",
    "    site = yearsites[i][5:10]\n",
    "    sites.append(site)\n",
    "\n",
    "df_yearsites = pd.DataFrame({'site': sites, 'year': years}, \n",
    "                            columns=['site', 'year'])\n",
    "df_yearsites = df_yearsites.sort_values(['site', 'year'])\n",
    "final_sites = list(set(df_yearsites.site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_yearsites.to_csv('../weadata/site_year_crithr1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "numofyears = list()\n",
    "\n",
    "for i in final_sites:\n",
    "    years = len(df_yearsites[df_yearsites[\"site\"] == i])\n",
    "    numofyears.append(years)\n",
    "    \n",
    "df_numofyears = pd.DataFrame({\"site\": final_sites,\n",
    "                              \"years\": numofyears})\n",
    "df_numofyears = df_numofyears.sort_values([\"site\"])\n",
    "df_numofyears = df_numofyears.reset_index().iloc[:, 1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_numofyears.to_csv(\"../weadata/site_years.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Select for subset dataset to include only valid (can be gap-filled) weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in required files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2125, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>03103</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>03103</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>03812</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>03812</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>03812</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    site  year\n",
       "0  03103  1964\n",
       "1  03103  1988\n",
       "2  03812  1973\n",
       "3  03812  1975\n",
       "4  03812  1979"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siteyear = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_year_crithr1.csv', dtype=\"str\")\n",
    "siteyear = siteyear.drop(siteyear.columns[0], axis=1)\n",
    "print(siteyear.shape)\n",
    "siteyear.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in individual weather data files and merge them into long form data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sites</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rh</th>\n",
       "      <th>solrad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961-05-01 00:00:00</th>\n",
       "      <td>03103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01 01:00:00</th>\n",
       "      <td>03103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01 02:00:00</th>\n",
       "      <td>03103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01 03:00:00</th>\n",
       "      <td>03103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01 04:00:00</th>\n",
       "      <td>03103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sites  temp  precip  rh  solrad\n",
       "date                                                \n",
       "1961-05-01 00:00:00  03103   NaN     NaN NaN     NaN\n",
       "1961-05-01 01:00:00  03103   NaN     NaN NaN     NaN\n",
       "1961-05-01 02:00:00  03103   NaN     NaN NaN     NaN\n",
       "1961-05-01 03:00:00  03103   NaN     NaN NaN     NaN\n",
       "1961-05-01 04:00:00  03103   NaN     NaN NaN     NaN"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 190923: still thinking whether the code below in this step 5 section is necessary now\n",
    "\n",
    "filelist = ['temp_all.csv', 'precip_all.csv', 'rh_all.csv', 'solrad_all.csv']\n",
    "weavars = ['temp', 'precip', 'rh', 'solrad']\n",
    "filenums = len(filelist)\n",
    "dfs = [[],[],[],[]]\n",
    "sites = sorted(list(set(siteyear.site)))\n",
    "grow_months = [5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "for i in range(filenums):\n",
    "    dfs[i] = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/' + filelist[i], index_col=0)\n",
    "    dfs[i].index = pd.to_datetime(dfs[i].index) # converting index timepoint into datetimeindex\n",
    "    dfs[i] = dfs[i][dfs[i].index.month.isin(grow_months)] # selecting only the growing season timepoints\n",
    "    dfs[i] = dfs[i].filter(items=sites, axis=1) # selcting only the valid sites\n",
    "    dfs[i][\"date\"] = dfs[i].index # creating another column that stores datetimeindex info, for melting purpose\n",
    "    dfs[i] = pd.melt(dfs[i], id_vars=\"date\", var_name=\"sites\", value_name=weavars[i])\n",
    "\n",
    "df = pd.merge(dfs[0], dfs[1])\n",
    "df = pd.merge(df, dfs[2])\n",
    "df = pd.merge(df, dfs[3])    \n",
    "\n",
    "df.index = pd.to_datetime(df.date)\n",
    "df = df.iloc[:, 1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting for valid site & years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_siteyears = siteyear.shape[0]\n",
    "frames = []\n",
    "\n",
    "for i in range(valid_siteyears):\n",
    "    dfi = df[df.index.year == int(siteyear.year[i])]\n",
    "    dfi = dfi[dfi.sites == siteyear.site[i]]\n",
    "    frames.append(dfi)\n",
    "#    print(len(frames)) # since this code takes time to run, \n",
    "                       # this helps to see where in the process it is\n",
    "\n",
    "df_valid = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving valid weather data frame into .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     sites  temp  precip        rh  solrad\n",
      "date                                                      \n",
      "1964-05-01 00:00:00  03103   9.4     0.0  0.605071     0.0\n",
      "1964-05-01 01:00:00  03103   5.6    15.0  0.850413     0.0\n",
      "1964-05-01 02:00:00  03103   4.4     0.0  0.925122     0.0\n",
      "1964-05-01 03:00:00  03103   3.3     0.0  0.924547     0.0\n",
      "1964-05-01 04:00:00  03103   2.8     0.0  0.924283     0.0\n",
      "                     sites  temp  precip        rh  solrad\n",
      "date                                                      \n",
      "1990-10-31 19:00:00  94910  21.1     0.0  0.543996     0.0\n",
      "1990-10-31 20:00:00  94910  22.8     0.0  0.470004     0.0\n",
      "1990-10-31 21:00:00  94910  23.3     0.0  0.440605     0.0\n",
      "1990-10-31 22:00:00  94910  22.8     0.0  0.454449     0.0\n",
      "1990-10-31 23:00:00  94910  19.4     0.0  0.562429     0.0\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.head())\n",
    "print(df_valid.tail())\n",
    "df_valid.to_csv(\"/home/disk/eos8/ach315/upscale/weadata/weadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Compile and gap-fill usable site-years data into individual weather data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JH191030: code is only for 'control' maizsim weather input at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time zone finder\n",
    "def find_zone(site):\n",
    "    \"\"\"\n",
    "    find time zone for specific sites\n",
    "    \"\"\"\n",
    "    lat = float(siteinfo[siteinfo.site == site].lat)\n",
    "    lon = float(siteinfo[siteinfo.site == site].lon) * -1\n",
    "    tf = TimezoneFinder()    \n",
    "    zone = tf.timezone_at(lng=lon, lat=lat)\n",
    "    return zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 200108JH: utc conversion second try!\n",
    "\n",
    "def utc_to_local(times, zone):\n",
    "    \"\"\"\n",
    "    convert list of utc timestamps into local time\n",
    "    \"\"\"\n",
    "    times = times.to_pydatetime() # convert from pd.DatetimeIndex into python datetime format\n",
    "    utc = pytz.timezone('UTC') # setting up the UTC timezone, requires package 'pytz'\n",
    "    local_datetime = list()\n",
    "    \n",
    "    for time in times:\n",
    "        utctime = utc.localize(time) # adding UTC timezone to datetime\n",
    "        localtime = utctime.astimezone(pytz.timezone(zone)) \n",
    "        datetime = pd.to_datetime(localtime)\n",
    "        local_datetime.append(datetime)\n",
    "        \n",
    "    return local_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8567.0"
      ]
     },
     "execution_count": 1370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_precip[local_start:local_end][site].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run time of code block ~52 mins\n",
    "count_start = datetime.datetime.now()\n",
    "\n",
    "### setting up growing season period\n",
    "season_start, season_end = '04-01', '10-31'\n",
    "\n",
    "### reading in required files\n",
    "siteinfo = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_info.csv', dtype='str', usecols=[1,2,3,4,5])\n",
    "siteyear = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/site_year_crithr1.csv', dtype='str', usecols=[1,2])\n",
    "siteyear = siteyear[siteyear.site != '41415'] # dropping out Guam within dataset\n",
    "\n",
    "### starting the loop to process each site-year\n",
    "for i in np.arange(siteyear.shape[0]): \n",
    "    ### selecting site-year combinations\n",
    "    site = siteyear.iloc[i,0]\n",
    "    year = siteyear.iloc[i,1]\n",
    "    print(site, year)\n",
    "    \n",
    "    ### constructing dataframe that will hold all weather data\n",
    "    col = ['jday','date','hour','solrad','temp','precip','rh', 'co2']\n",
    "    df_wea = pd.DataFrame(columns=col)\n",
    "\n",
    "    ### setting up for time-relating entries\n",
    "    times = pd.date_range(season_start + '-' + str(year), \n",
    "                              season_end + '-' + str(year)+ ' 23:00:00', freq='1H') # utc time\n",
    "    zone = find_zone(site)\n",
    "    local_datetime = utc_to_local(times, zone)\n",
    "            \n",
    "    ### selecting weather data\n",
    "    utc_start, utc_end = str(times[0]), str(times[-1])\n",
    "    df_wea.temp = list(df_temp[utc_start:utc_end][site])\n",
    "    df_wea.rh = list(np.round((df_rh[utc_start:utc_end][site])*100, 2))\n",
    "    df_wea.precip = list(df_precip[utc_start:utc_end][site]/10) #*** this is a temporary fix for precip scaling bug\n",
    "    df_wea.co2 = 400    \n",
    "\n",
    "    ### selecting solar radiation \n",
    "    t1 = pd.to_datetime(utc_start).to_pydatetime()\n",
    "    t2 = pd.to_datetime(utc_end).to_pydatetime()\n",
    "    tdiff = t2-t1\n",
    "    local_start = str(local_datetime[0])[:19] \n",
    "    local_end = str(pd.to_datetime(local_start).to_pydatetime() + tdiff)[:19]\n",
    "    df_wea.solrad = list(df_solrad[local_start:local_end][site])\n",
    "\n",
    "    ### adding time-relating info to data frame\n",
    "    local = pd.date_range(local_start, local_end, freq='H')\n",
    "    df_wea.jday = local.dayofyear\n",
    "    df_wea.date = local.strftime(\"'%m/%d/%Y'\")\n",
    "    df_wea.hour = local.hour    \n",
    "    \n",
    "    ### gap-filling weather data\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        # creating a log file that documents the number of missing data for each site-year\n",
    "        f = open('/home/disk/eos8/ach315/upscale/weadata/data/log.txt', 'a+')\n",
    "        f.write('site: %s' %siteyear.iloc[i,:][0])\n",
    "        f.write(', year: %s' %siteyear.iloc[i,:][1])\n",
    "        f.write(', gap-filled: %s\\r\\n' %df_wea.isna().sum().sum())\n",
    "        f.close()\n",
    "        \n",
    "        # gap-filling data by linearly interpolating with data from hour before and after\n",
    "        df_wea = df_wea.interpolate() \n",
    "        \n",
    "    ### saving individual site-year weather file into .csv \n",
    "    df_wea.to_csv('/home/disk/eos8/ach315/upscale/weadata/data/control/' + site + '_' + year + '.txt', sep='\\t', index=False)\n",
    "    \n",
    "count_end = datetime.datetime.now()\n",
    "diff = count_end - count_start\n",
    "print('run time:', diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Final step of gap-filling if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pd.interpolate() cannot gap-fill missing data if the missing data is located at the very beginning of the data (nan in first row), the code is just to check whether there are site-years with that situation, and if so assigns the missing data in the first row a default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob('/home/disk/eos8/ach315/upscale/weadata/data/control/*')\n",
    "\n",
    "for name in fnames: \n",
    "    df_wea = pd.read_csv(name)\n",
    "    df_wea = df_wea.drop(df_wea.columns[0], axis=1)\n",
    "    if df_wea.isna().sum().sum() > 0:\n",
    "        print(name.split('/')[-1], df_wea.isna().sum().sum())\n",
    "\n",
    "# no files required additional gap-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: reading in a final compiled weather file to check output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/disk/eos8/ach315/upscale/weadata/data/control/24127_1989.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jday</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>solrad</th>\n",
       "      <th>temp</th>\n",
       "      <th>precip</th>\n",
       "      <th>rh</th>\n",
       "      <th>co2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>'1989/04/01'</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>'1989/04/01'</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>'1989/04/01'</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>'1989/04/01'</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>'1989/04/01'</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   jday          date  hour  solrad  temp  precip    rh  co2\n",
       "0    91  '1989/04/01'     0     0.0   7.8     0.0  0.76  400\n",
       "1    91  '1989/04/01'     1     0.0   5.6    13.0  0.85  400\n",
       "2    91  '1989/04/01'     2     0.0   6.1    25.0  0.89  400\n",
       "3    91  '1989/04/01'     3     0.0   6.1     0.0  0.86  400\n",
       "4    91  '1989/04/01'     4     0.0   6.1     0.0  0.82  400"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
